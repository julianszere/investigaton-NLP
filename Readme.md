# Memoria para asistentes conversacionales

## Introducción

Este repositorio acompaña el track de NLP y plantea el desafío de ampliar un asistente conversacional (chatbot) con un módulo de memoria.

Incluimos una referencia básica basada en un *semantic retriever*. Recordemos que un retriever semántico puede pensarse como una función que, para una consulta `Q` y un conjunto de documentos `D`, devuelve la relevancia de cada documento condicionada a la consulta.

La utilización de este proyecto es completamente opcional: pueden usarlo tal cual, adaptarlo o simplemente tomarlo como fuente de inspiración.

## Estructura del proyecto

La carpeta principal es `src`. Allí van a encontrar:

- **`models`**: implementaciones de referencia. `LiteLLM` simplifica la prueba de múltiples APIs al unificar su interfaz. Si trabajan con modelos de Hugging Face sugerimos Qwen3, que ofrece buen *reasoning* y soporte de *tools*; por eso incluimos `QwenModel`. Dependiendo del hardware y la experiencia, también pueden evaluar Ollama o vLLM.
- **`agents`**: distintos agentes ya configurados. `JudgeAgent` evalúa si la respuesta es correcta. `FullContextAgent` envía la instancia completa de LongMemEval a un modelo con ventana de contexto amplia (por ejemplo GPT-5 o Gemini); es una alternativa directa pero costosa y poco creativa. `RAGAgent` implementa el módulo de RAG que usamos para el benchmark.
- **`datasets`**: utilidades para cargar y representar el benchmark. Incluye la clase `LongMemEvalInstance`, alineada con la definición del paper.

```python
def instance_from_row(self, row):
    return LongMemEvalInstance(
        question_id=row["question_id"],
        question=row["question"],
        sessions=[
            Session(session_id=session_id, date=date, messages=messages)
            for session_id, date, messages in zip(
                row["haystack_session_ids"], row["haystack_dates"], row["haystack_sessions"]
            )
        ],
        t_question=row["question_date"],
        answer=row["answer"],
    )
```

- **`experiments`**: implementación del pipeline experimental y utilidades para cargar modelos y módulos de memoria. En `config` pueden definir qué agente usar (`fullcontext`, `rag`, etc.), qué modelo responde, qué modelo actúa como juez y otros parámetros.


## Setup

Recomendamos utilizar `uv` para gestionar el entorno. Podés descargarlo e instalarlo desde <https://docs.astral.sh/uv/getting-started/installation/>.

### Instalación de dependencias

Una vez instalado `uv`, sincronizá las dependencias:

```sh
uv sync
```

Es posible que `torch` y `transformers` no se instalen automáticamente para permitirles elegir versiones específicas (por ejemplo, con soporte CUDA). La instalacion mas basica es mediante:

```
uv pip install torch transformers
```

### Descarga de datasets

Con el entorno configurado, descargá los datasets del benchmark:

```sh
uv run scripts/download_dataset.py
```

Alternativamente, activá el entorno virtual y ejecutá el script manualmente:

```sh
source .venv/bin/activate
python scripts/download_dataset.py
```

### Descarga de embeddings

Uno de los módulos de memoria incluidos utiliza embeddings. Para cada mensaje se calcula un embedding:

[formula]

Luego se utiliza para realizar *retrieval*:

[ejemplo query, retrieval, respuesta]

Incluimos embeddings precomputados para acelerar las primeras ejecuciones. Para descargarlos, ejecutá:

```sh
TODO
```

### API Keys

Si vas a correr el benchmark con una API externa, configurá un archivo `.env` con la variable `OPENAI_API_KEY` (o la clave que corresponda a tu proveedor).

### Correr el benchmark

Para ejecutar el benchmark:

```sh
uv run main.py
```

o bien:

```sh
python main.py
```

### Analizar resultados

En `notebooks/rag_result_eval.ipynb` encontrarás un análisis general de los resultados, segmentado por tipo de pregunta. Recomendamos reportar las métricas siguiendo esa segmentación, ya que cada categoría presenta distintos niveles de dificultad.
